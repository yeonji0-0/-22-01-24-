{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "> the collective name for a set of language modeling and feature learning techniques in NLP where words and phrases from the vocabulary are mapped to vectors of real numbers. (from Wikipedia)\n",
    "\n",
    "> 수학적으로, 고차원의 공간을 더 낮은 공간으로 변환하는 방법(embedding)과 같은 의미이기도 하다.\n",
    "\n",
    "> 결국, 고차원으로 표현된 feature vector(local representation, BOW, TF-IDF 등)을 distributional semantic을 가지는 vector space에 mapping 시켜주는 방법이다.\n",
    "\n",
    "> <b>\"You shall know a word by the company it keeps\"(John R. Firth, 1957)<b>, it called \"Distributed Hypothesis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![word embedding fig](figs/word-vector-space-similar-words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize word vectors](figs/visualize-word-vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 최근 많이 쓰이는 word embedding 방법들이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wevi : word embedding visual inspector\n",
    "    \n",
    "> https://ronxin.github.io/wevi/\n",
    "\n",
    "이론을 정리하기 위해 체험을 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 현재 word embedding이 핫하게 된 시작 알고리즘. \"Distributed representations of words and phrases and their compositionality(NIPS 2013)\" 에 처음 소개되었다.\n",
    "\n",
    "> Reference : https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skip-gram](figs/skip-gram.png)\n",
    "![simple-skip-gram](figs/simple-skip-gram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Skip-Gram with Negative Sampling, 줄여서 SGNS라고 부르며 Neural Net을 이용한 word embedding이 빠르게 구현가능해진 이유기도 하다.\n",
    "\n",
    "> Negative Sampling이란, 마지막 단계의 softmax를 구하는 문제를 주변 단어(postive class)와 무작위로 골라진 나머지 단어들(negative class)로 분류하는 binary classfication 문제로 바꿔주는 기법이며 이를 통해 굉장히 빠르게 word embedding 수행이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/delabgpu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package punkt to /home/delabgpu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "sentences = [list(sent) for sent in movie_reviews.sents()]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim에서 word2vec 불러오기\n",
    "\n",
    "# 학습 모델 구현.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.5213533043861389),\n",
       " ('commendable', 0.49510741233825684),\n",
       " ('gutsy', 0.48329102993011475),\n",
       " ('fantastic', 0.4828444719314575),\n",
       " ('lousy', 0.47579044103622437),\n",
       " ('dopey', 0.47346508502960205),\n",
       " ('darned', 0.4671739935874939),\n",
       " ('meaty', 0.4588979184627533),\n",
       " ('qualify', 0.4554523825645447),\n",
       " ('untalented', 0.4544796049594879)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가, 비슷한 단어 찾기.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GloVe는 Gloval Vectors의 약자로, aggregated global word co-occurence statistics를 최적화하는 방향으로 학습하는 word embedding 방법이다. \"GloVe: Gloval Vectors for Word Representation(EMNLP 2014)\"에 소개되었다.\n",
    "\n",
    "> Reference : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![glove](figs/glove.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove는 현재 파이썬으로는 구현이 힘듭니다, 사용을 위해서는 c++을 사용하셔야 합니다.\n",
    "#from glove import Glove\n",
    "\n",
    "#glove_model = Glove(no_components=100, learning_rate=0.05)\n",
    "#glove_model.fit(sentences, epochs=10, no_threads=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 현재 NLP task에서 word embedding의 baseline으로 사용되는 기법이다. subword embedding model, char n-gram embedding model이라고도 한다.\n",
    "\n",
    "> word2vec을 만들었던, Tomas Mikolov가 Google에서 Facebook으로 옮긴 뒤에 낸 모델로 word2vec의 단점을 보완한 모델이다.\n",
    "\n",
    "> word2vec의 단점이었던, OOV 문제와 low frequency를 많이 해결하였다.\n",
    "\n",
    "> word를 subword 단위로 표현하는 것으로 기본적으로 SGNS 방식이다.\n",
    "\n",
    "> Reference : https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![char3-grams](figs/char3-grams.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim에서 FastText 불러오기\n",
    "\n",
    "\n",
    "# FastText 학습.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goods', 0.5055395364761353),\n",
       " ('goodnight', 0.43297654390335083),\n",
       " ('great', 0.41480714082717896),\n",
       " ('goofball', 0.40592318773269653),\n",
       " ('goose', 0.39415624737739563),\n",
       " ('bad', 0.3819223642349243),\n",
       " ('marvellous', 0.36979711055755615),\n",
       " ('goo', 0.35548409819602966),\n",
       " ('gutsy', 0.34930139780044556),\n",
       " ('glorified', 0.3475170135498047)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 평가.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elmo_sesame_street](figs/elmo_sesame_street.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ELMo는 Embeddings from Language Model의 약자입니다. ELMo는 pre-trained language model을 사용하여 문맥에 맞는 word embedding, \"Contextualized Word Embedding\"을 만드는 방법입니다.\n",
    "\n",
    "> bidirectional Language Model을 이용하여, pre-trained embedding vector를 corpus의 context(syntax, semantics, polysemy) 정보를 보완해주는 embedding vector를 만들어 준다.\n",
    "\n",
    "> tensorflow, pytorch를 통해서 bidirectional LSTM model을 만들어 사용이 가능하다. (이미 구현된 model이 github에 공개되어있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Deep contextualized word representations(NAACL 2018)\"에 소개된 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# tensorflow_hub에서 데이터 불러오기.\n",
    "\n",
    "\n",
    "# 가까운 미래에는 실행이 되길 빌면서..\n",
    "\n",
    "embeddings = elmo(\n",
    "    [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"module_apply_default/aggregation/mul_3:0\", shape=(2, 6, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference : https://allennlp.org/elmo, https://github.com/allenai/bilm-tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elmo_architecture](figs/elmo_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![elmo_model](figs/elmo_model.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
